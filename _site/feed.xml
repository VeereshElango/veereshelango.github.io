<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>VeereshElango</title>
    <description>Data Scientist &amp; Developer</description>
    <link>http://localhost:4000/</link>
    <atom:link href="http://localhost:4000/feed.xml" rel="self" type="application/rss+xml"/>
    <pubDate>Wed, 05 Jul 2017 14:08:45 +0200</pubDate>
    <lastBuildDate>Wed, 05 Jul 2017 14:08:45 +0200</lastBuildDate>
    <generator>Jekyll v3.5.0</generator>
    
      <item>
        <title>Can Machine write Thirukkural ?</title>
        <description>&lt;p&gt;Inspired by Andrej Karpathy’s blog post &lt;a href=&quot;http://karpathy.github.io/2015/05/21/rnn-effectiveness/&quot;&gt;The Unreasonable Effectiveness of Recurrent Neural Networks&lt;/a&gt;, I was super excited to apply the LSTM and see the results on a different dataset.&lt;/p&gt;

&lt;p&gt;Before we dive in I thought to give a simple intro to RNN and LSTM.&lt;/p&gt;

&lt;h2 id=&quot;short-intro-about-rnn-and-lstm&quot;&gt;Short intro about RNN and LSTM&lt;/h2&gt;
&lt;p&gt;Recurrent Neural Network is a neural network model in which each hidden layer receives both input from the previous layer and 
input from itself at each step. This enables it to hold information across inputs which can be thought as a memory.&lt;/p&gt;

&lt;p&gt;The problem of Recurrent Neural Network is that its memory is very short term.
This is solved in Long Short Term Memory networks (LSTM).&lt;/p&gt;

&lt;p&gt;Long Short Term Memory (LSTM) networks are kind of recurrent neural network which are capable of learning long-term dependencies. 
LSTM has a cell state which runs through all the modules of the neural network.
The cell state is very easy for the information to flow along it unchanged.
There are four gates which regulate the addition and removal of information from the cell state.&lt;/p&gt;

&lt;p&gt;As LSTM holds the information for long term, it can be trained with text file at character level as input, so that it learns to predict the next character in the sequence.
 The other interesting applictions of this RNN/LSTM model are music generation, image captioning, language translator and ever writing bible.&lt;/p&gt;

&lt;p&gt;For further understanding about this neural networks, check the &lt;a href=&quot;#references&quot;&gt;references&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;about-dataset---thriukkural&quot;&gt;About Dataset - Thriukkural&lt;/h2&gt;
&lt;p&gt;&lt;a href=&quot;https://en.wikipedia.org/wiki/Tirukku%E1%B9%9Ba%E1%B8%B7&quot;&gt;Thirukkural&lt;/a&gt; is one of the most prominent and celebrated works in Tamil Literature. It is also one of the
most widely translated non-religious works in the world.
It is written by the poet &lt;a href=&quot;https://en.wikipedia.org/wiki/Thiruvalluvar&quot;&gt;Thiruvalluvar&lt;/a&gt; who lived in the 6th century. It is a unique ethical guide which delivers code of 
 conduct to the mankind to follow for all time to come. 
In total, there are 1330 couplets which are divided into 133 sections with 10 couplets each.
Each couplet has exactly 7 words, 4 in one line and 3 in next.&lt;/p&gt;

&lt;h2 id=&quot;why-this-dataset&quot;&gt;Why this dataset?&lt;/h2&gt;
&lt;p&gt;Since childhood, I was admired how he expresses great morals in just 7 words.
 He wrote only 1330 couplets which almost delivers all the essential morals
   for a person to lead a successful life.
This provoked the curiosity within me to know how it would be if Thiruvalluvar has written more Thirukkural.
   Crazy thought !!&lt;/p&gt;

&lt;h2 id=&quot;about-the-model&quot;&gt;About the model&lt;/h2&gt;
&lt;p&gt;The first step is to create the dataset. I found a Thirukkural literature in json format. I parsed the json file
 and created a text file with 1330 couplets alone.&lt;/p&gt;

&lt;p&gt;The next step is to fed this file as input to LSTM model. I took the sample
&lt;a href=&quot;https://github.com/fchollet/keras/blob/master/examples/lstm_text_generation.py&quot;&gt;code&lt;/a&gt; from Keras Github repository and made changes to adapt to this dataset.&lt;/p&gt;

&lt;p&gt;It is a simple model with single LSTM layer with 128 neurons. It is fed with very small
data of 1330 couplets with each of 7 words, in total 9310 words. 
The model is trained for 20 iterations.&lt;/p&gt;

&lt;p&gt;The code is available &lt;a href=&quot;https://github.com/VeereshElango/text_generation_thirukkural&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;results&quot;&gt;Results&lt;/h2&gt;

&lt;p&gt;The results definitely show Thiruvalluvar cannot be replaced by Machine anytime soon. But if you notice the results, astonishedly the 
machine (LSTM) has learnt to produce sensible Tamil words with punctuations. I was also amazed that how this 
  simple model learn the syntax of Thirukkural such as nearly four words in the first line and three words in the 
  second line.&lt;/p&gt;

&lt;p&gt;Start Word : “சாதலின் “&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;சாதலின் இல்லாய் நினிபன் பெருந்தகை 
சிலையார் யாணென்னும் பயர்.
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;சாதலின்ற் பொருட்செல்வம் தாண்டற்றும் மருந்து 
கேணல் சிறப்பின் இரந்து.
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;p&gt;Start Word : “   காதல்”&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;காதல்ல்லாது செய்யார் முன்றி தீயும் 
அருவியல் போர் கேது.
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;காதல்டு யாதெனின் நெஞ்சத்தான் கண்ணுடையார் க
ேதலிற் கொள்வாரோட்க்கு விருந்து.
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;p&gt;Start Word : “  காமம் “&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;காமம்ன்தும் இடும்பை பஇடைச் சிறப்பின் இரவுருமாடு 
ஒரிவது குறவினும் பெறும்.
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;காமம்ணிடும் பெறுப்சின் செய்யும் அவர்தென்பீர் 
கண்ட செல்வம் தரும்.
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;Start Word : “ இறைவன் “&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;இறைவன் பத் தலையபின் துணையாகை 
நீக்கிக் காமந்த நூர்.
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;இறைவன் முன்றினும் உய்வாத்தல் இருவறுப் 
துணையாது விழைந்து விடல்.
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;Start Word : “  ஒழுக்க”&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;ஒழுக்கத் தலையபின் துணையாது நீங்கி 
விருமுங்க்கல்ல செய்யார் மாத்து.
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;Start Word : “ உயர்ந்த”&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;உயர்ந்த தஒழில் தூழ்கச் சிறப்பின் 
இரந்தவற்று விகர்முடியார் அனைத்து.
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;உயர்ந்த யஇருள் நோக்கெச் சுடைத்து வேநீால் 
வடிவார் என்றார் அனைத்து.
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;Start Word : “ பெருமை “&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;பெருமை எய்தார் முற்றியது எண்ணிக் 
கூடியமை என்னும் செருக்கு.
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;பெருமை கூற்றம் காதல் அல்லசெய்தல் 
இன்னை அருமாப் படும்.
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;h2 id=&quot;references&quot;&gt;References&lt;/h2&gt;
&lt;h4 id=&quot;rnnlstm&quot;&gt;RNN/LSTM&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;http://karpathy.github.io/2015/05/21/rnn-effectiveness/&quot;&gt;The Unreasonable Effectiveness of Recurrent Neural Networks&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://colah.github.io/posts/2015-08-Understanding-LSTMs/&quot;&gt;Understanding LSTM Networks&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://iamtrask.github.io/2015/11/15/anyone-can-code-lstm/&quot;&gt;Anyone Can Learn To Code an LSTM-RNN in Python (Part 1: RNN)&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://medium.com/@shiyan/understanding-lstm-and-its-diagrams-37e2f46f1714&quot;&gt;Understanding LSTM and its diagrams&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://www.hexahedria.com/2015/08/03/composing-music-with-recurrent-neural-networks/&quot;&gt;Composing Music With Recurrent Neural Networks&lt;/a&gt;
    &lt;h4 id=&quot;thirukkural&quot;&gt;Thirukkural&lt;/h4&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.britannica.com/topic/Tirukkural&quot;&gt;Thirukkural - by Britannica&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.google.com/url?sa=t&amp;amp;rct=j&amp;amp;q=&amp;amp;esrc=s&amp;amp;source=web&amp;amp;cd=2&amp;amp;cad=rja&amp;amp;uact=8&amp;amp;sqi=2&amp;amp;ved=0ahUKEwic78TMjfLUAhXLfRoKHUSHDmgQFggxMAE&amp;amp;url=https%3A%2F%2Fen.wikipedia.org%2Fwiki%2FTirukku%25E1%25B9%259Ba%25E1%25B8%25B7&amp;amp;usg=AFQjCNHgAu5JoMvimy5DIy6LL3p1Wl10Lg&quot;&gt;Thirukkural - by Wikipedia&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
        <pubDate>Sun, 02 Jul 2017 00:00:00 +0200</pubDate>
        <link>http://localhost:4000/articles/2017-07/text-generation-with-thirukkural</link>
        <guid isPermaLink="true">http://localhost:4000/articles/2017-07/text-generation-with-thirukkural</guid>
        
        
        <category>LSTM</category>
        
      </item>
    
  </channel>
</rss>
